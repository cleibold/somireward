{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a053aac-8007-4982-8d97-5436be32096a",
   "metadata": {},
   "source": [
    "# Get modified activity (binned spike counts) by subtracting possible speed (and acceleration) modulation\n",
    "\n",
    "We estimate the modulation with linear regression of the measured activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6458ff5-5f6c-4546-808c-7c4d92955e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# My module for the decoding analysis\n",
    "#import decoding\n",
    "# from decoding import *\n",
    "from decoding import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8874614-7faf-4f5e-b3f1-d3d072f6ea69",
   "metadata": {},
   "source": [
    "## Set paths and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47714c02-c4d8-47aa-9bd8-eb5b9e4b375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_p = Path(os.getcwd())\n",
    "start_p = base_p / 'data'\n",
    "print(start_p)\n",
    "\n",
    "# Load linear modelling data:\n",
    "fname = 'linear_modelling_of_SOMI_activity_during_reward.npy'\n",
    "\n",
    "data_d = np.load(start_p / fname, allow_pickle=True).item() \n",
    "print(data_d.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd71ceb2-6a5c-49ff-bcc5-12d744661736",
   "metadata": {},
   "source": [
    "## Set parameters from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340e9f13-dc6b-43c0-94ff-38f2929a1626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time windows for regression and simulated data;\n",
    "# before and after the reward delievery start:\n",
    "time_before = 3.0 # in s\n",
    "time_after = time_before # in s\n",
    "\n",
    "decoding_offset_time = 1.5 # in s\n",
    "# decoding_offset = round(decoding_offset_time * sample_rate_Hz)\n",
    "\n",
    "# maximum time window for binning before and after decoding start time:\n",
    "window_max = 1.5 # in s\n",
    "\n",
    "# Dict keys for expert and non-expert sessions:\n",
    "expert_key = 'high'\n",
    "non_expert_key = 'low'\n",
    "\n",
    "params_d = data_d['params']\n",
    "print(params_d)\n",
    "\n",
    "num_bins = params_d['# bins']\n",
    "binwidth_steps = params_d['binwidth']\n",
    "binwidth = binwidth_steps/sample_rate_Hz # binwidth in s\n",
    "bw_ms = round(binwidth_steps/sample_rate)\n",
    "\n",
    "print('Bin width:', binwidth)\n",
    "ibw = 1/binwidth\n",
    "\n",
    "num_bins_before = round(time_before/binwidth) # should be 1/2 of num_bins\n",
    "print('Number of bins before:', num_bins_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d0aa68-c555-442a-8f23-be879ac07cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aligned time points of the bins (centers):\n",
    "ts, t_bin = np.linspace(-time_before,time_after,num_bins, endpoint=False, retstep=True)\n",
    "print(ts)\n",
    "ts_center = ts + 0.5*t_bin\n",
    "# print(ts_center)\n",
    "num_bins_decoding = np.searchsorted(ts, -decoding_offset_time)\n",
    "print('Number of bins for decoding:', num_bins_decoding, ts[num_bins_decoding], ts[2*num_bins_decoding])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8685ac6f-ebf1-4b59-81ce-25da879aa95d",
   "metadata": {},
   "source": [
    "## Select expert or non-expert sessions/animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d60981-f64c-41eb-a2af-c42a28ed2f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_s = expert_s\n",
    "\n",
    "# Get data from Expert or Non-expert sessions:\n",
    "if session_s.startswith(expert_s):\n",
    "    session_status = expert_s\n",
    "    lm_d = data_d[expert_key]\n",
    "elif session_s.startswith(non_expert_s):\n",
    "    session_status = non_expert_s\n",
    "    lm_d = data_d[non_expert_key]\n",
    "    \n",
    "lm_d.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a4da92-8885-4866-b8da-0db58c6b94c2",
   "metadata": {},
   "source": [
    "### Relevant lists in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c483e2-97bc-4cfb-a6e4-5e6f0a6cb473",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_ids = lm_d['cell ids'].copy()\n",
    "\n",
    "# Data as lists of the same length as cell_ids\n",
    "X_list = lm_d['X'] # just references to the list in the dict lm_d\n",
    "y_list = lm_d['y']\n",
    "\n",
    "# print(len(cell_ids), len(X_list), len(y_list))\n",
    "cell_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de003621-86d4-4052-92ae-4dfa7de9caa3",
   "metadata": {},
   "source": [
    "## Iterate over the lists and fit the linear model\n",
    "\n",
    "We need to store and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb232e-6bab-4e17-bb23-3e824f951bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression variables in the arrays X\n",
    "labels = lm_d['coef legend']\n",
    "print(labels)\n",
    "acc_ind = labels.index('acceleration') # should be == 3\n",
    "print(acc_ind)\n",
    "speed_ind = labels.index('speed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e77335-c5ae-419d-b888-8ffa9acc1472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_round(x):\n",
    "    \"\"\"Stochastic rounding of numpy array x to nearest integer.\"\"\"\n",
    "    return np.floor(x + rng.random(size=x.shape)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c099d0dc-b8c8-42c4-a1c7-2b1cfd209c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "rng_state = rng.bit_generator.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef8dea7-1f1b-473a-ace6-a61d9f111b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_data_d = {} # container for loaded and modified data\n",
    "# from all cells under consideration\n",
    "\n",
    "num_cells = 0\n",
    "for i, cell in enumerate(cell_ids):\n",
    "    unit_data_d = {} # dictionary for data of a unit\n",
    "    \n",
    "    #print(i, cell)\n",
    "    # Match date pattern in session name:\n",
    "    res = re.search(r\"20\\d{2}-\\d{2}-\\d{2}\", cell) \n",
    "    session_id_s = session_s + '__' + res.string[:res.end()] #.replace('/','__')\n",
    "    #print('Session ID:', session_id_s)\n",
    "    unit_id_s = cell[cell.find('shank'):].removesuffix('.txt')\n",
    "    #print('Session ID:', session_id_s, 'Unit ID:', unit_id_s)\n",
    "    cell_id_s = '__'.join([session_id_s, unit_id_s])\n",
    "    print('Cell:', cell_id_s)\n",
    "\n",
    "    unit_data_d['cell_id_s'] = cell_id_s\n",
    "    unit_data_d['unit_id_s'] = unit_id_s\n",
    "    unit_data_d['session_id_s'] = session_id_s\n",
    "\n",
    "    # Regression data\n",
    "    Xo = X_list[i].copy()\n",
    "    yo = y_list[i].copy()\n",
    "    # in original shape\n",
    "    #print(Xo.shape, yo.shape)\n",
    "    # yts = np.reshape(yo,(-1,num_bins))\n",
    "    \n",
    "    num_trials = yo.shape[0]//2\n",
    "    #print('Number of trials:', num_trials)\n",
    "    unit_data_d['num_trials'] = num_trials\n",
    "    unit_data_d['num_reward_start'] = num_trials\n",
    "\n",
    "    # Clean up nans in X:\n",
    "    num_nans = np.zeros(Xo.shape[0], dtype=int)\n",
    "    X_mean = np.zeros(Xo.shape[0])\n",
    "    for i, x in enumerate(Xo):\n",
    "        num_nans[i] = np.isnan(x).sum()\n",
    "        if num_nans[i] > 0:\n",
    "            X_mean[i] = np.nanmean(x)\n",
    "            #x = np.nan_to_num(x, nan=X_mean[i])\n",
    "            np.nan_to_num(x, nan=X_mean[i], copy=False)\n",
    "            # Changes the nan entries in x in place, and thus in Xo, \n",
    "            # with the mean of the non-nan entries.\n",
    "            # Then, we can continue as before.\n",
    "            # However, we should check if there are more than\n",
    "            # a few nans in the array! Let's say more than 6, or so?\n",
    "        else:\n",
    "            X_mean[i] = np.mean(x)\n",
    "    #print(num_nans)\n",
    "    #print('Cleaned up average of X:', X_mean)\n",
    "    \n",
    "    # Further preparation for regression:\n",
    "    Xo[acc_ind] *= sample_rate_Hz # Change units of acceleration, in-place\n",
    "    X_mean[acc_ind] *= sample_rate_Hz\n",
    "    X_center = Xo.T - X_mean # center predictor variables \n",
    "    ys = yo.flatten('C') # makes a copy!\n",
    "    #print(ys.shape) # should be equal: Xo.shape[1] == ys.shape[0]\n",
    "\n",
    "    # average firing rate of binned spike counts\n",
    "    pooled_rate = ibw*np.mean(ys) # in Hz\n",
    "    if pooled_rate < min_pooled_rate:\n",
    "        print(f'Pooled average rate of unit {unit_id_s} is only {pooled_rate:5.2f} Hz.')\n",
    "        print('Exclude from decoding analysis.')\n",
    "        continue # to next cell\n",
    "    unit_data_d['pooled_rate'] = pooled_rate\n",
    "\n",
    "    # Finally, fit the linear model:\n",
    "    lm = LinearRegression()\n",
    "    # Use centered X for the regression:\n",
    "    lm.fit(X_center, ys)\n",
    "    lm_coeffs = lm.coef_\n",
    "    lm_intercept = lm.intercept_ # equal to np.mean(ys) due to centering\n",
    "    unit_data_d['lm_coeffs'] = lm_coeffs\n",
    "    unit_data_d['lm_intercept'] = lm_intercept\n",
    "    \n",
    "    #y_pred = lm.predict(X_center)\n",
    "\n",
    "    # fig = plt.figure(dpi=150)\n",
    "    # plt.plot(ys)\n",
    "    # plt.plot(y_pred)\n",
    "    # plt.plot(X_center[:,acc_ind]*coeffs[acc_ind], color='tab:red') # speed contribution\n",
    "    # #plt.xlim([0, 200])\n",
    "    # plt.show(fig)\n",
    "    # plt.close(fig)\n",
    "\n",
    "    # What do we want to subtract?\n",
    "    # #- X_center[:,speed_ind]*lm_coeffs[speed_ind] - X_center[:,acc_ind]*lm_coeffs[acc_ind]\n",
    "    # Subtract modulation of ys due to speed and accelaration variation\n",
    "    # as estimated by the fitted linear (regression) model:\n",
    "    ys_red = ys - (X_center[:,speed_ind]*lm_coeffs[speed_ind] + X_center[:,acc_ind]*lm_coeffs[acc_ind])\n",
    "    \n",
    "    # Add preprocessed data to the dicts in cells_data_d\n",
    "    unit_data_d['bw_ms'] = bw_ms\n",
    "    unit_data_d['bin_width'] = binwidth\n",
    "    unit_data_d['yts'] = np.reshape(ys, (-1, num_bins))\n",
    "    unit_data_d['yts_red'] = np.reshape(ys_red, (-1, num_bins))\n",
    "    # the reduced spike counts include some randomness:\n",
    "    #unit_data_d['yts_red_counts'] = np.reshape(ys_red_counts, (-1, num_bins))\n",
    "     \n",
    "    # Put the loaded data into the container\n",
    "    cells_data_d[cell_id_s] = unit_data_d\n",
    "    num_cells += 1\n",
    "\n",
    "print(f'Data from {num_cells} cells have been loaded.', len(cells_data_d))\n",
    "print('Now, (almost) all relevant date should be in the dict cells_data_d.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f6b70-9f75-4ea5-809d-a8d1434c9c06",
   "metadata": {},
   "source": [
    "## Prepare spike count data for population decoding\n",
    "\n",
    "First we generate the spike count data to be used for the decoding analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9474348-3b31-43bb-83ff-56196461b00c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_repeat = 1 # number of repetitions for stochastic rounding\n",
    "\n",
    "num_cells = 0\n",
    "for cell_id_s, cell_d in cells_data_d.items():\n",
    "    print('Cell:', cell_id_s)\n",
    "    print('Pooled rate:', cell_d['pooled_rate'])\n",
    "    num_cells += 1\n",
    "\n",
    "    num_trials = cell_d['num_trials']\n",
    "    # Get true labels: 0 for \"before\" and 1 for \"after\"\n",
    "    true_labels = get_true_labels(num_trials)\n",
    "    #print(num_trials, cell_d['yts'].shape)\n",
    "    \n",
    "    cell_d['num_bins'] = num_bins_decoding\n",
    "    # This should be correct!\n",
    "    # Before it was set here as\n",
    "    #num_bins_decoding = num_bins_before//2\n",
    "    # gets generated by \n",
    "    #create_bins(bin_width, window_max)\n",
    "    \n",
    "    cell_d['true_labels'] = true_labels\n",
    "\n",
    "    # So far missing for decoding from multiple units\n",
    "    #data_d['spike_counts_aligned'] = spike_counts_aligned\n",
    "    #data_d['rate_counts_aligned'] = rate_counts_aligned\n",
    "\n",
    "    spike_counts_list = [] # collect the spike counts\n",
    "    # rng_state = rng.bit_generator.state\n",
    "    for i in range(num_repeat+1):\n",
    "        if i == 0:\n",
    "            spike_counts_aligned = cell_d['yts'][:,:2*num_bins_decoding].astype(int)           \n",
    "        else:\n",
    "            # rng.bit_generator.state = rng_state\n",
    "            yts_red = cell_d['yts_red'][:,:2*num_bins_decoding]\n",
    "            spike_counts_aligned = stochastic_round(np.maximum(yts_red, 0.0))\n",
    "        #print(i, spike_counts_aligned.shape)\n",
    "        pooled_rate_rep = ibw*np.mean(spike_counts_aligned) # in Hz\n",
    "        print(pooled_rate_rep)\n",
    "        spike_counts_list.append(spike_counts_aligned)\n",
    "\n",
    "    cell_d['spike_counts_list'] = spike_counts_list\n",
    "    cell_d['num_repeat'] = num_repeat\n",
    "\n",
    "print(f'Data from {num_cells} cells has been modified.', len(cells_data_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bfb252-faf6-425a-8f3b-8581bb978b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_data_fname = 'activity_subtracted_data__' + session_s + '.npy'\n",
    "fname = start_p / cells_data_fname\n",
    "\n",
    "np.save(fname, cells_data_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502fe953-df11-471e-b424-94594ff83052",
   "metadata": {},
   "source": [
    "## Decoding analysis for all cells that were not excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3238dde-b815-47c6-9861-770ff98b9c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window lengths for analysis:\n",
    "win_start = 0.25 #0.25 # in seconds\n",
    "win_stop = 1.5 #3.75 should be less than (or equal to) window_max\n",
    "win_step = binwidth #max(0.125, bin_width) #0.125 in seconds, 2**-3\n",
    "win_arr = create_windows(win_start, win_stop, win_step)\n",
    "print(win_arr)\n",
    "\n",
    "split_variant = 0\n",
    "split_variant_name = split_variant_names[split_variant]\n",
    "print('Using split variant:', split_variant_name)\n",
    "decoding_s = f'_bw{bw_ms}' + f'_split_{split_variant_name}'\n",
    "# print(decoding_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d9d790-d549-464c-9c53-c6cbd2b125b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rng.bit_generator.state = rng_state\n",
    "\n",
    "print_output = False\n",
    "\n",
    "cells_results_d = {} # container for results of decoding analysis\n",
    "num_cells = 0\n",
    "for cell_id_s, cell_d in cells_data_d.items():\n",
    "    # if num_cells < 0:\n",
    "    #     num_cells += 1\n",
    "    #     continue\n",
    "    # elif num_cells > 15:\n",
    "    #     break\n",
    "    print('Cell:', cell_id_s)\n",
    "    print('Pooled rate:', cell_d['pooled_rate'])\n",
    "    num_cells += 1\n",
    "    #continue\n",
    "\n",
    "    num_trials = cell_d['num_trials']\n",
    "    # Get true labels: 0 for \"before\" and 1 for \"after\"\n",
    "    true_labels = cell_d['true_labels'] #get_true_labels(num_trials)\n",
    "    \n",
    "    # Split into even and odd trials:\n",
    "    # if split_variant == 0:\n",
    "    inds_split = split_even_odd(num_trials)\n",
    "\n",
    "    results_list = [] # collect the decoding results\n",
    "    # rng_state = rng.bit_generator.state\n",
    "    for i in range(num_repeat+1):\n",
    "        # if i == 0:\n",
    "        #     spike_counts_aligned = cell_d['yts'][:,:2*num_bins_decoding].astype(int)           \n",
    "        # else:\n",
    "        #     # rng.bit_generator.state = rng_state\n",
    "        #     yts_red = cell_d['yts_red'][:,:2*num_bins_decoding]\n",
    "        #     spike_counts_aligned = stochastic_round(np.maximum(yts_red, 0.0))\n",
    "        # print(i, ':', np.abs(cell_d['spike_counts_list'][i] - spike_counts_aligned).sum())\n",
    "        spike_counts_aligned = cell_d['spike_counts_list'][i]\n",
    "\n",
    "        pooled_rate = ibw*np.mean(spike_counts_aligned) # in Hz\n",
    "        #print(pooled_rate)\n",
    "        \n",
    "        # Perform decoding analysis for a collection of time windows:\n",
    "        # 3 variants in this order: frac_correct, frac_correct_rate_before, frac_correct_train\n",
    "        fc_arr, pv_arr = decoding_windows(win_arr, binwidth, num_bins_decoding, \n",
    "                                          spike_counts_aligned, spike_counts_aligned, \n",
    "                                          true_labels, inds_split, output=print_output)\n",
    "\n",
    "        results_d = dict(repeat = i, win_arr=win_arr, fc_arr=fc_arr, pv_arr=pv_arr, \n",
    "                          bw_ms=bw_ms, split_variant=split_variant, \n",
    "                          bin_width=binwidth, \n",
    "                         session_rate=cell_d['pooled_rate'], pooled_rate=pooled_rate)\n",
    "        \n",
    "        results_list.append(results_d)\n",
    "\n",
    "    # Store the results in a container\n",
    "    cells_results_d[cell_id_s] = results_list\n",
    "    \n",
    "    # Plot results in decoding_results_list\n",
    "    fig, ax = plt.subplots(1,1, figsize=(3,2.5), dpi=150)\n",
    "    for res in results_list:\n",
    "        ax.plot(res['win_arr'], res['fc_arr'][:,0], marker='.', label=res['repeat'])\n",
    "        \n",
    "    ax.axhline(0.65, linestyle=':', linewidth=0.5, color='k')\n",
    "    ax.set_xlabel('window length (s)')\n",
    "    ax.set_title(cell_id_s)\n",
    "    ax.set_ylabel('fraction correct')\n",
    "    ax.legend()\n",
    "    plt.show(fig)\n",
    "\n",
    "print(f'Data from {num_cells} cells has been analyzed.', len(cells_results_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b52d7a4-bb33-49cb-9d48-04915e01e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cells_results_d.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7b6f05-ee6b-4f41-a9bc-28ee52fcfc1a",
   "metadata": {},
   "source": [
    "## Store the dictionary, indicating the session status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f7a916-febb-4993-bd28-bf5bb7e17456",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_results_fname = 'activity_subtracted_results__' + session_s + '.npy'\n",
    "fname = start_p / cells_results_fname\n",
    "\n",
    "np.save(fname, cells_results_d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
